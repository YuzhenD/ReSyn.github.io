<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exploring Real&Synthetic Dataset and Linear Attention in Image Restoration</title>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Exploring Real&Synthetic Dataset and Linear Attention in Image Restoration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Yuzhen Du</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Teng Hu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Jiangning Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ran Yi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Lizhuang Ma</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShangHai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Yutu Lab</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image restoration (IR), which aims to recover high-quality images from degraded inputs, is a crucial task in modern image processing. 
            Recent advancements in deep learning, particularly with Convolutional Neural Networks (CNNs) and Transformers, have significantly 
            improved image restoration performance. However, existing methods lack a unified training benchmark that specifies the training iterations 
            and configurations. Additionally, we construct an image complexity evaluation metric using the gray-level co-occurrence matrix (GLCM) and 
            find that there exist a bias between the image complexity distributions of commonly used IR training and testing datasets, leading to 
            suboptimal restoration results. Therefore, we construct a new large-scale IR dataset called ReSyn, that utilizes a novel image filtering 
            method based on image complexity to achieve a balanced image complexity distribution, and contains both real and AIGC synthetic images. 
            From the perspective of measuring the model's convergence ability and restoration capability, we construct a unified training standard that 
            specifies the training iterations and configurations for image restoration models. Furthermore, we explore how to enhance the performance 
            of transformer-based image restoration models based on linear attention mechanism. We propose RWKV-IR, a novel image restoration model that 
            incorporates the linear complexity RWKV into the transformer-based image restoration structure, and enables both global and local receptive 
            fields. Instead of directly integrating the Vision-RWKV into the transformer architecture, we replace the original Q-Shift in RWKV with a 
            novel Depth-wise Convolution shift, which effectively models the local dependencies, and is further combined with Bi-directional attention 
            to achieve both global and local aware linear attention. Moreover, we propose a Cross-Bi-WKV module that combines two Bi-WKV modules with 
            different scanning orders to achieve a balanced attention for horizontal and vertical directions. Extensive quantitative and qualitative 
            experiments demonstrate the effectiveness and competitive performance of our RWKV-IR model.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Datasets Analysis. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">ReSyn Dataset</h2>
          <p>
            The diversity analysis of our ReSyn dataset. It contains both real and synthetic images from a variety of
            data sources and covers a wide range of resolutions.
          </p>
          
        </div>
      </div>
      <!--/ Datasets Analysis. -->

      <!-- Dataset Images -->
      <div class="column">
        <h2 class="title is-3">Some Images from ReSyn Dataset</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Some images from our ReSyn Dataset, contain real and synthetic images.
            </p>
            
          </div>

        </div>
      </div>
    </div>
    <!--/ Dataset Images -->

    <!-- Complexity Analysis. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Image Complexity Analysis</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Complexity Distribution</h3>
        <div class="content has-text-justified">
          <p>
            The complexity distributions of different datasets. The complexity distributions of the training datasets DIV2K and DF2K
            have a typical shift, containing more images of low complexity. Our ReSyn dataset balances the distribution of low and 
            high complexity images by image filtering based on the newly proposed GLCM image complexity measure.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src=""
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Urban100 Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src=""
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">Manga109 Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src=""
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">BS100 Dataset</p>
          </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Complexity formula-->
        <div class="formula">
            <label>Image Complexity Formulaï¼š</label>I<sub>Complexity<\sub> =  ENT - ENE + DISS<br>
        </div>
        <!--/ Complexity formula-->
        
        <!-- PSNR-Complexity. -->
        <h3 class="title is-4">Compleity-PSNR Relation Analysis</h3>
        <div class="content has-text-justified">
          <p>
            Using image complexity, you could determine whether the image is hard to restore.
          </p>
        </div>
        <div class="content has-text-centered">
          PSNR (x2 SR on Urban100) performance can be predicted by the proposed GLCM image complexity and BPP. For each predictor, we 
          sort images and compute the Pearson correlation (rho) with PSNR. Compared to BPP, GLCM Complexity has a higher correlation 
          to PSNR.
        </div>
        <!--/ PSNR-Complexity. -->

      </div>
    </div>

  </div>
</section>

</html>
