<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Exploring Real&Synthetic Dataset and Linear Attention in Image Restoration</title>
    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./css/bulma.min.css">
  <link rel="stylesheet" href="./css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./css/bulma-slider.min.css">
  <link rel="stylesheet" href="./css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./js/fontawesome.all.min.js"></script>
  <script src="./js/bulma-carousel.min.js"></script>
  <script src="./js/bulma-slider.min.js"></script>
  <script src="./js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Exploring Real&Synthetic Dataset and Linear Attention in Image Restoration</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a>Yuzhen Du</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Teng Hu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="">Jiangning Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="">Ran Yi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Lizhuang Ma</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ShangHai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Youtu Lab</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Image restoration (IR), which aims to recover high-quality images from degraded inputs, is a crucial task in modern image processing. 
            Recent advancements in deep learning, particularly with Convolutional Neural Networks (CNNs) and Transformers, have significantly 
            improved image restoration performance. However, existing methods lack a unified training benchmark that specifies the training iterations 
            and configurations. Additionally, we construct an image complexity evaluation metric using the gray-level co-occurrence matrix (GLCM) and 
            find that there exist a bias between the image complexity distributions of commonly used IR training and testing datasets, leading to 
            suboptimal restoration results. Therefore, we construct a new large-scale IR dataset called ReSyn, that utilizes a novel image filtering 
            method based on image complexity to achieve a balanced image complexity distribution, and contains both real and AIGC synthetic images. 
            From the perspective of measuring the model's convergence ability and restoration capability, we construct a unified training standard that 
            specifies the training iterations and configurations for image restoration models. Furthermore, we explore how to enhance the performance 
            of transformer-based image restoration models based on linear attention mechanism. We propose RWKV-IR, a novel image restoration model that 
            incorporates the linear complexity RWKV into the transformer-based image restoration structure, and enables both global and local receptive 
            fields. Instead of directly integrating the Vision-RWKV into the transformer architecture, we replace the original Q-Shift in RWKV with a 
            novel Depth-wise Convolution shift, which effectively models the local dependencies, and is further combined with Bi-directional attention 
            to achieve both global and local aware linear attention. Moreover, we propose a Cross-Bi-WKV module that combines two Bi-WKV modules with 
            different scanning orders to achieve a balanced attention for horizontal and vertical directions. Extensive quantitative and qualitative 
            experiments demonstrate the effectiveness and competitive performance of our RWKV-IR model.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Datasets Analysis. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">ReSyn Dataset</h2>
          <p>
            The diversity analysis of our ReSyn dataset. It contains both real and synthetic images from a variety of
            data sources and covers a wide range of resolutions.
          </p>
           <img src="./figs/per.png"
                 class="interpolation-image"
            />
        </div>
      </div>
      <!--/ Datasets Analysis. -->

      <!-- Dataset Images -->
      <div class="column">
        <h2 class="title is-3">Some Images from ReSyn Dataset</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              Some images from our ReSyn Dataset, contain real and synthetic images.
            </p>
            <img src="./figs/dataset.png"
                 class="interpolation-image"
            />
          </div>

        </div>
      </div>
    </div>
    <!--/ Dataset Images -->

    <!-- Complexity Analysis. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Image Complexity Analysis</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Complexity Distribution</h3>
        <div class="content has-text-justified">
          <p>
            The complexity distributions of different datasets. The complexity distributions of the training datasets DIV2K and DF2K
            have a typical shift, containing more images of low complexity. Our ReSyn dataset balances the distribution of low and 
            high complexity images by image filtering based on the newly proposed GLCM image complexity measure.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./figs/Urban100.png"
                 class="interpolation-image"
                 />
            <p>Urban100 Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./figs/Manga109.png"
                 class="interpolation-image"
                 />
            <p class="is-bold">Manga109 Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./figs/BS100.png"
                 class="interpolation-image"
                 />
            <p class="is-bold">BS100 Dataset</p>
          </div>
        </div>
        <div class="column is-full-width">
        <h2 class="title is-3">Image Complexity Analysis</h2>

        <!-- Interpolating. -->
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./figs/DIV2K.png"
                 class="interpolation-image"
                 />
            <p>DIV2K Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./figs/DF2K.png"
                 class="interpolation-image"
                 />
            <p class="is-bold">DF2K Dataset</p>
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./figs/ReSyn.png"
                 class="interpolation-image"
                 />
            <p class="is-bold">ReSyn Dataset</p>
        </div>
        </div>
        <br/>
        <!--/ Interpolating. -->

        <!-- Complexity formula-->
        <div class="formula">
            <label>Image Complexity Formulaï¼š</label>I<sub>Complexity</sub> =  ENT - ENE + DISS<br>
        </div>
        <!--/ Complexity formula-->
        
        <!-- PSNR-Complexity. -->
        <h3 class="title is-4">Compleity-PSNR Relation Analysis</h3>
        <div class="content has-text-justified">
          <p>
            Using image complexity, you could determine whether the image is hard to restore.
          </p>
        <img src="./figs/11.png"
                 class="interpolation-image"
                 />
        </div>
        <div class="content has-text-centered">
          PSNR (x2 SR on Urban100) performance can be predicted by the proposed GLCM image complexity and BPP. For each predictor, we 
          sort images and compute the Pearson correlation (rho) with PSNR. Compared to BPP, GLCM Complexity has a higher correlation 
          to PSNR.
        </div>
        <!--/ PSNR-Complexity. -->

        <!-- Dataset Form -->
        <div class="column">
        <h2 class="title is-3">Data Collection and Shuffle</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              The images used for image restoration model training need to have a high pixel-level quality. 
              To this end, we divide the shuffle process into three steps. 1) Firstly, the images of resolution 
              smaller than 800x800 are discarded, since for super-resolution tasks, the images need to be down-sampled. 
              This can help remove most low-quality images. 2) Secondly, to remove the blurry or noisy degraded images,
              we follow the blur and noise suppression process proposed in LSDIR. The remaining images are under blur 
              detection by the variance of the image Laplacian, and flat region detection through the Sobel filter. 3) 
              Thirdly, all the images are shuffled through the GLCM complexity metric (detailed below) to ensure a 
              balanced distribution. We ensure that the number of images with complexity values below zero is equal 
              to that above zero. Therefore, we can form a dataset of balanced image complexity distribution. It should 
              be mentioned that images from different sources are filtered individually.
            </p>
            <div class="column has-text-centered">
                <img src="./figs/procedure.png"
                     class="interpolation-image"
                     />
                <p class="is-bold">Final Image Complexity Shuffle</p>
            </div>
          </div>

        </div>
      </div>
          
        <!-- / Dataset Form -->
          
      </div>
    </div>

  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- RWKVIR Framwork -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Model Framework</h2>
          <p>
            Our framework consists of three stages: shallow feature extraction, deep feature enhancement, and HQ image reconstruction. 
            For deep feature enhancement, a series of Global&Local Linear attention Layers (GLLL, which is based on RWKV) and a Conv 
            Block is used. Each GLLL layer contains several GLLB blocks, which contain linear complexity attention and a channel mix module.
          </p>
          <img src="./figs/Fram.png"
               class="interpolation-image"/
              >
        </div>
        </div>
      </div>
      <!--/ RWKVIR Framwork -->

    <!-- RWKV Block -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">RWKVIR Sub Block</h2>

       <div class="content">
          <h3 class="title is-3">DC-Shift</h3>
            <img src="./figs/DCshift.png"
                 class="interpolation-image"
                 />
            <p>Different shift methods. The Q-shift is a simple channel replacement operation using 
               four neighboring pixels, while our DC-shift is a depth-wise conv leveraging the 
               surrounding pixels in a kxk neighborhood.</p>
          </div>
           <div class="content">
          <h3 class="title is-3">Cross-Bi-WKV</h3>
            <img src="./figs/cwkv.png"
                 class="interpolation-image"
                 />
            <p>Illustration of Cross-Bi-WKV, which consists of two cross scanning Bi-WKV modules.</p>
          </div>
        </div>
        </div>
        <br/>
        <!--/ RWKV Block -->

        <!-- Experiments -->
    <h3 class="title is-4">Compleity-PSNR Relation Analysis</h3>
        <div class="content has-text-justified">
        <h2 class="title is-3">Super-Resolution Experiments</h2>
            
        <!-- Interpolating. -->
        <h3 class="title is-4">Classical SR Experiments(100K training iters)</h3>
        <div class="columns is-centered has-text-centered">
            <img src="./figs/SR_1.png"
                 class="interpolation-image"
                 /> 
          </div>
          <div class="content has-text-centered">
              Quantitative comparison of classic image super-resolution with 
               state-of-the-art methods on 10K iters training. The best and 
               the second-best results are in red and blue.
          </div>
            
          <h3 class="title is-4">Classical SR Experiments(500K training iters)</h3>
          <div class="columns is-centered has-text-centered">
            <img src="./figs/SR_2.png"
                 class="interpolation-image"
                 />
            </div>
            <div class="content has-text-centered">  
            Quantitative comparison on classic image 
                super-resolution with state-of-the-art methods on 500K iters training.
            </div>
            
          <h3 class="title is-4">LightWeight SR Experiments(50K training iters)</h3>
        <div class="columns is-centered has-text-centered">
            <img src="./figs/SR_3.png"
                 class="interpolation-image"
                 />
            </div>
            <div class="content has-text-centered"> 
            Quantitative comparison on lightweight image super-resolution with 
                state-of-the-art methods on 50K training iterations. The best is in red.
          </div>
          <h3 class="title is-4">LightWeight SR Experiments(500K training iters)</h3>
         <div class="columns is-centered has-text-centered">
            <img src="./figs/SR_4.png"
                 class="interpolation-image"
                 />
            </div>
            <div class="content has-text-centered">
            Quantitative comparison on lightweight image super-resolution
                with state-of-the-art methods on 500K training iters.
          </div>
        </div>
        <br/>
        <!--/ Experiments -->
        
      </div>
    </div>

  </div>
</section>


    

</html>
